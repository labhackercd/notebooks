{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualmente, o algoritmo se tornou popular na área de Aprendizado de Máquina (Machine Learning) para **categorizar textos baseado na frequência das palavras usadas**, e assim pode ser usado para identificar se determinado e-mail é um SPAM ou sobre qual assunto se refere determinado texto, por exemplo.\n",
    "\n",
    "Por ser muito simples e rápido, possui um desempenho relativamente maior do que outros classificadores. Além disso, o Naive Bayes só precisa de um pequeno número de dados de teste para concluir classificações com uma boa precisão.\n",
    "\n",
    "A principal característica do algoritmo, e também o motivo de receber “naive” (ingênuo) no nome, é que **ele desconsidera completamente a correlação entre as variáveis (features)**. Ou seja, se determinada fruta é considerada uma “Maçã” se ela for “Vermelha”, “Redonda” e possui “aproximadamente 10cm de diâmetro”, o algoritmo não vai levar em consideração a correlação entre esses fatores, tratando cada um de forma independente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "from nltk.corpus import floresta\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from string import digits, punctuation\n",
    "from pprint import pprint\n",
    "from random import shuffle\n",
    "import stop_words\n",
    "import textblob\n",
    "import re\n",
    "import nltk\n",
    "import requests\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook vamos tentar aplicar o classificador Naive Bayes para classificar discursos dos parlamentares da Câmara dos Deputados proferidos em plenário. Os dados utilizados para análise foram obtidos através do [Babel](https://dev.babel.labhackercd.leg.br/), um repositórios de dados de manifestações políticas, e podem ser acessados pelo *endpoint*: https://dev.babel.labhackercd.leg.br/api/v1/manifestations?manifestation_type__id=2 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 discursos coletados\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('https://dev.babel.labhackercd.leg.br/api/v1/manifestations?manifestation_type__id=2')\n",
    "data = response.json()['results']\n",
    "response = requests.get('https://dev.babel.labhackercd.leg.br/api/v1/manifestations?manifestation_type__id=2&page=10')\n",
    "data += response.json()['results']\n",
    "response = requests.get('https://dev.babel.labhackercd.leg.br/api/v1/manifestations?manifestation_type__id=2&page=20')\n",
    "data += response.json()['results']\n",
    "\n",
    "speeches = []\n",
    "for speech in data:\n",
    "    for attr in speech['attrs']:\n",
    "        if attr['field'] == 'original':\n",
    "            speeches.append(attr['value'])\n",
    "            break\n",
    "\n",
    "print(len(speeches), 'discursos coletados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para limpar texto do discurso tirando as notas do taquigrafo\n",
    "def clear_speech(text):\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r'[OA] SRA?[\\w\\s.]+-', '', text)\n",
    "    text = re.sub(r'PRONUNCIAMENTO[\\sA-Z]+\\s', '', text)\n",
    "#     text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s[\\.\\\"]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[Vv]\\.[Ee][Xx][Aa]\\.', 'v.exa', text)\n",
    "    text = re.sub(r'[Aa][Rr][Tt]\\.', 'art', text)\n",
    "    text = re.sub(r'[Ss][Rr][Ss]?\\.', 'sr', text)\n",
    "    text = re.sub(r'[Ss][Rr][Aa][Ss]?\\.', 'sr', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "cleaned_text = clear_speech('. '.join(speeches))\n",
    "blob = textblob.TextBlob(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentenças\n",
    "\n",
    "Para realizar a classificação de um discurso, ele será divido em sentenças, utilizando a biblioteca TextBlob, e cada sentença será classificada individualmente e o conjunto de sentenças classificadas (uma sentença não precisa ser, necessariamente, classificada) definirá a classificação de um discurso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presidente!\n",
      "\n",
      "O PSDB é a favor da votação.. sr Presidente Deputado Eduardo Cunha, sr e sr Parlamentares, a PEC  é um acerto da Federação, ela faz parte do pacto federativo.\n",
      "\n",
      "Pela primeira vez, nós teremos uma estação de tratamento de água, haverá produção de água com qualidade, para a população da cidade de Óbidos.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(blob.sentences[23], end='\\n\\n')\n",
    "print(blob.sentences[233], end='\\n\\n')\n",
    "print(blob.sentences[100], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "*Stopwords* (ou palavras de parada – tradução livre) são palavras que podem ser consideradas irrelevantes para o conjunto de resultados a ser exibido em uma busca realizada em uma search engine. Exemplos: as, e, os, de, para, com, sem, foi. Esse conjunto de palavras pode variar em função do contexto em que será utilizado.\n",
    "\n",
    "Para esse notebook serão utilizadas as palavras fornecidas pela biblioteca NLTK, além de outras palavras selecionadas de acordo com os textos utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stop_words.get_stop_words('portuguese') + [\n",
    "    'presidente', ',', '.', '...', 'é', 'questão', 'art', 'ordem', 'v.exa', ':', 'governo', 'sr', 'agência', 'º',\n",
    "    'aqui', 'vai', 'artigo', '§', 'neste', 'vamos', 'agora', \"''\", 'fazer', 'mesa', 'ainda', 'porque', 'trata',\n",
    "    'estrutura', 'sobre', 'então', 'todos', 'obstrução', 'votação', 'presença', 'deputados', 'vou', 'brasil',\n",
    "    'discutir', 'vigência', 'colocar', 'regimento', 'momento', ';', 'dois', 'dessa', 'medida', 'proposta',\n",
    "    'casa', 'matéria', 'queria', 'assim', 'possamos', 'microfone', 'certeza', 'hoje', 'profissional', 'deixar',\n",
    "    'provisória', 'ora', 'base', 'importante', 'veto', 'fala', '!', 'ministério', 'aumento', 'inciso',\n",
    "    'manifestação', 'xiii', 'diálogo', 'podemos', 'apenas', 'poder', 'efeitos', 'pode', 'acordo', 'solicitação',\n",
    "    'reflexão', '?', 'ausência', 'aprovada', 'lideranças', 'dizer', 'bancada', 'portanto', 'peço', 'recolher',\n",
    "    'prática', 'pois', 'democracia', 'milhões', 'bilhões', 'melhoria', 'atividade', 'claro', 'saber', 'dar',\n",
    "    'avanço', 'condições', 'desastre', 'especialmente', 'exatamente', 'política', 'vezes', 'fazê-lo', 'têm',\n",
    "    'derrubar', 'precisa', 'custo', 'necessária', 'cláusula', 'proposição', '-', 'palavra', 'tempo', 'segundos',\n",
    "    'fez', 'necessário', 'zero', 'interesse', 'srs', 'sr', 'sras', 'sra', 'deputado', 'presidente', 'é', 'nº',\n",
    "    's.a.', 'v.exa.', 'v.exa', '#', 'anos', 'º', 'exa', 'mesa', 'legislatura', 'sessão', 'maioria', 'seguinte',\n",
    "    'mandato', 'bilhões', 'quilômetros', 'ª', 'parabéns', 'membros', 'convido', 'usual', 'biênio',\n",
    "    'brasil', 'palavra', 'discussão', 'período', 'início', 'pronunciamento', 'suplente', 'atividade', 'ação',\n",
    "    'ações', 'daqueles', 'diferenças', 'pasta', 'milhares', 'srªs', 'emenda', 'àqueles', 'tamanha', 'mês',\n",
    "    'capaz', 'km', 'modelo', 'tarefas', 'colegas', 'programa', 'voz', 'meios de comunicação', 'pronunciamento',\n",
    "    'casa', 'sessão', 'deliberativa', 'solene', 'ordinária', 'extraordinária', 'encaminhado', 'orador', 'tv',\n",
    "    'divulgar', 'deputado', 'parlamento', 'parlamentar', 'projeto', 'proposta', 'requerimento', 'destaque',\n",
    "    'veto', 'federal', 'câmara', 'senado', 'congresso', 'nacional', 'país', 'estado', 'brasil', 'lei',\n",
    "    'política', 'povo', 'voto', 'partido', 'liderança', 'bancada', 'bloco', 'líder', 'lider', 'frente',\n",
    "    'governo', 'oposição', 'presença', 'presente', 'passado', 'ausência', 'ausencia', 'ausente', 'obstrução',\n",
    "    'registrar', 'aprovar', 'rejeitar', 'rejeição', 'sabe', 'matéria', 'materia', 'questão', 'ordem', 'emenda',\n",
    "    'sistema', 'processo', 'legislativo', 'plenário', 'pedir', 'peço', 'comissão', 'especial', 'permanente',\n",
    "    'apresentar', 'encaminhar', 'encaminho', 'orientar', 'liberar', 'apoiar', 'situação', 'fato', 'revisão',\n",
    "    'tempo', 'pauta', 'discutir', 'discussão', 'debater', 'retirar', 'atender', 'colegas', 'autor', 'texto',\n",
    "    'medida', 'união', 'república', 'audiência', 'audiencia', 'público', 'publico', 'reunião', 'agradecer',\n",
    "    'solicitar', 'assistir', 'contrário', 'favorável', 'pessoa', 'comemorar', 'ato', 'momento', 'diretora',\n",
    "    'possível', 'atenção', 'agradeço', 'naquele', 'necessárias', 'presidenta', 'compromisso', 'geradas',\n",
    "    'primeiro', 'simplesmente', 'ideal', 'argumento', 'i', 'válido', 'envolvidos', 'nesse', 'aspecto',\n",
    "    'existentes', 'normativo', 'irá', 'nada', 'melhor', 'esperarmos', 'pouco', 'resolvermos', 'problema',\n",
    "    'postura', 'faltas', 'declara', '%', 'grande', 'dia', 'obrigado', 'agradeço', 'agradecido', 'população',\n",
    "    'maior', 'cada', 'bem', 'mundo', 'desta', 'mil', 'sendo', 'outros', '$', '!', '@', '#', '&', '(', ')', 'sim',\n",
    "    'r', 'sempre', 'além', 'semana', 'relação', 'onde', 'meio', 'inclusive', 'lá', 'vem', 'menos', 'menor',\n",
    "    'qualquer', 'desde', 'ontem', 'hoje', 'exemplos', 'exemplo', 'tão', 'fim', 'janeiro', 'fevereiro', 'março',\n",
    "    'abril', 'maio', 'junho', 'julho', 'agosto', 'setembro', 'outubro', 'novembro', 'dezembro', 'alguns', 'tudo',\n",
    "    'durante', 'gostaria', 'três', 'conta', 'feito', 'através', 'antes', 'depois', 'verdade', 'bom', 'quase',\n",
    "    'setor', 'aí', 'disse', 'principalmente', 'final', 'vão', 'coisa', 'ver', 'sentido', 'nova', 'vários', 'novo',\n",
    "    'nenhuma', 'quanto', 'infelizmente', 'felizmente', 'número', 'duas', 'dois', 'tanto', 'acho', 'achar',\n",
    "    'enquanto', 'deve', 'apelo', 'papel', 'últimos', 'faço', 'fazer', 'garantir', 'garantia', 'fica', 'obrigado',\n",
    "    'obrigado..', 'assunto', 'sido', 'vir', 'incrementar', 'central', 'aproximado', 'aproximadamente',\n",
    "    'hipotética', 'hipotese', 'hipótese', 'média', 'superior', 'superiores', 'gerais', 'venha', 'minas'\n",
    "]\n",
    "stopwords += [x for x in punctuation]\n",
    "stem_stopwords = [stemmer.stem(word) for word in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def simplify_tag(tag):\n",
    "    if \"+\" in tag:\n",
    "        return tag[tag.index(\"+\") + 1:]\n",
    "    else:\n",
    "        return tag\n",
    "\n",
    "floresta_twords = floresta.tagged_words()\n",
    "for (word, tag) in floresta_twords:\n",
    "    tag = simplify_tag(tag)\n",
    "    words = word.casefold().split('_')\n",
    "    if tag not in ('adj', 'n', 'prop', 'nprop', 'est', 'npro'):\n",
    "        stopwords += [stemmer.stem(word) for word in words]\n",
    "\n",
    "stopwords = list(set(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras mais comuns\n",
      "[('rio', 422), ('segurança', 242), ('trabalho', 228), ('municípios', 221), ('educação', 220), ('sociedade', 189), ('saúde', 188), ('recursos', 182), ('cidade', 170), ('intervenção', 167), ('vida', 160), ('trabalhadores', 156), ('direito', 147), ('crise', 141), ('reforma', 137), ('município', 136), ('pec', 130), ('petrobras', 123), ('justiça', 122), ('ministro', 120), ('reais', 119), ('defesa', 119), ('social', 118), ('constituição', 117), ('lotéricos', 116), ('lula', 113), ('prefeito', 112), ('sul', 111), ('dilma', 110), ('dinheiro', 107), ('empresas', 105), ('civil', 104), ('paulo', 103), ('previdência', 101), ('polícia', 99), ('respeito', 98), ('luta', 95), ('família', 94), ('deus', 92), ('econômica', 91), ('tribunal', 89), ('comunicação', 89), ('violência', 89), ('renda', 88), ('corrupção', 88), ('desenvolvimento', 87), ('pt', 87), ('boa', 87), ('escola', 85), ('decisão', 84), ('escolas', 84), ('professores', 83), ('responsabilidade', 83), ('serviço', 83), ('carnaval', 83), ('crime', 81), ('preço', 80), ('área', 80), ('empresa', 79), ('decreto', 79), ('cidadão', 79), ('direitos', 78), ('caixa', 77), ('tribuna', 74), ('construção', 73), ('sociais', 73), ('militar', 73), ('fundo', 72), ('oportunidade', 72), ('água', 71), ('fundamental', 71), ('capital', 70), ('senhores', 70), ('..', 69), ('pagar', 69), ('agricultura', 68), ('ensino', 68), ('crianças', 67), ('mercado', 66), ('história', 66), ('economia', 65), ('ninguém', 65), ('presidente..', 64), ('serviços', 64), ('nome', 64), ('famílias', 64), ('bahia', 63), ('prefeitos', 62), ('cunha', 62), ('psol', 62), ('cidades', 62), ('golpe', 62), ('áreas', 61), ('acesso', 61), ('dentro', 61), ('estadual', 60), ('categoria', 60), ('maranhão', 60), ('servidores', 59), ('hora', 59)]\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.tokenize.word_tokenize(cleaned_text)\n",
    "words = [word.casefold() for word in all_words if stemmer.stem(word) not in stem_stopwords]\n",
    "dist = nltk.FreqDist(words)\n",
    "print('Palavras mais comuns')\n",
    "print(dist.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esse notebook, usaremos a implementação do TextBlob para classificar as sentenças. No TextBlob, podemos definir um método de extração de atributos (*feature extraction*) que são utilizados para comparar dois textos. Esse método deve receber como parâmetro o texto, pode receber um segundo argumento que contem os dados de treinamento e deve retornar um dicionário de atributos do texto recebido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'real': True,\n",
       " 'calcul': True,\n",
       " 'tax': True,\n",
       " 'patrimôni': True,\n",
       " 'rend': True,\n",
       " 'aplic': True,\n",
       " 'alíquot': True,\n",
       " 'simul': True,\n",
       " 'val': True}"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "def freq_feature_extractor(document):\n",
    "    document = regex.sub(' ', document.casefold())\n",
    "    tokens = nltk.tokenize.word_tokenize(document)\n",
    "    tokens = [stemmer.stem(token) for token in tokens if stemmer.stem(token) not in stem_stopwords]\n",
    "    dist = nltk.FreqDist(tokens)\n",
    "    \n",
    "    features = {\n",
    "        stem: True\n",
    "#         stem: dist.freq(stem)\n",
    "        for stem, _ in dist.most_common()\n",
    "    }\n",
    "    return features\n",
    "\n",
    "freq_feature_extractor('Calcula-se que a taxação de grandes patrimônios poderia render aproximadamente  bilhões de reais por ano se aplicada uma alíquota média de % sobre os bens das pessoas, em uma simulação hipotética, sobre valores superiores a  milhão de reais.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados de treinamento\n",
    "\n",
    "A planilha de treinamento está disponível no [Google Drive](https://docs.google.com/spreadsheets/d/1qmJjRexlSUYOAN12IY_82NyOUaLZlbBsoeYRuulEjFE/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_RELATION = {\n",
    "    0: 'none',\n",
    "    1: 'adm-publica',\n",
    "    2: 'agricultura-pecuaria-pesca-extrativismo',\n",
    "    3: 'arte-cultura-religiao',\n",
    "    4: 'cidades-desenvolvimento-urbano',\n",
    "    5: 'ciencia-tecnologia-inovacao',\n",
    "    6: 'ciencias-exatas',\n",
    "    7: 'ciencias-sociais',\n",
    "    8: 'comunicacoes',\n",
    "    9: 'defesa-seguranca',\n",
    "    10: 'direito-civil',\n",
    "    11: 'direito-constitucional',\n",
    "    12: 'direito-consumidor',\n",
    "    13: 'direito-justica',\n",
    "    14: 'direito-penal',\n",
    "    15: 'direitos-humanos-minorias',\n",
    "    16: 'economia',\n",
    "    17: 'educacao',\n",
    "    18: 'energia-recursos-hidricos-minerais',\n",
    "    19: 'esporte-lazer',\n",
    "    20: 'estrutura-fundiaria',\n",
    "    21: 'financas-publicas-orcamento',\n",
    "    22: 'homenagens-datas-comemorativas',\n",
    "    23: 'industria-comercio-servicos',\n",
    "    24: 'meio-ambiente-desenvolvimento-sustentavel',\n",
    "    25: 'politica-partidos-eleicoes',\n",
    "    26: 'previdencia-assistencia-social',\n",
    "    27: 'processo-legislativo-atuacao-parlamentar',\n",
    "    28: 'relacoes-internacionais-comercio-exterior',\n",
    "    29: 'saude',\n",
    "    30: 'trabalho-emprego',\n",
    "    31: 'turismo',\n",
    "    32: 'viacao-transporte-mobilidade',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento de conteúdo: 210 sentenças\n",
      "Teste de conteúdo: 90 sentenças\n",
      "Treinamento de temas: 147 sentenças\n",
      "Teste de temas: 63 sentenças\n"
     ]
    }
   ],
   "source": [
    "theme_data = []\n",
    "content_data = []\n",
    "\n",
    "with open('naive-bayes-train.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        for idx, category in enumerate(row[1:-1]):\n",
    "            if category != '' and idx > 0:\n",
    "                theme_data.append((row[0], LABELS_RELATION[idx]))\n",
    "                content_data.append((row[0], 'content'))\n",
    "            elif category != '' and idx == 0:\n",
    "                content_data.append((row[0], 'useless'))\n",
    "\n",
    "shuffle(content_data)\n",
    "content_data_size = len(content_data)\n",
    "content_train = content_data[:math.floor(content_data_size * 0.7)]\n",
    "content_test = content_data[math.floor(content_data_size * 0.7):]\n",
    "print('Treinamento de conteúdo: {} sentenças'.format(len(content_train)))\n",
    "print('Teste de conteúdo: {} sentenças'.format(len(content_test)))\n",
    "\n",
    "shuffle(theme_data)\n",
    "theme_data_size = len(theme_data)\n",
    "theme_train = theme_data[:math.floor(theme_data_size * 0.7)]\n",
    "theme_test = theme_data[math.floor(theme_data_size * 0.7):]\n",
    "print('Treinamento de temas: {} sentenças'.format(len(theme_train)))\n",
    "print('Teste de temas: {} sentenças'.format(len(theme_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_classifier = NaiveBayesClassifier(content_train, feature_extractor=freq_feature_extractor)\n",
    "content_classifier.accuracy(content_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031746031746031744"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_classifier = NaiveBayesClassifier(theme_train, feature_extractor=freq_feature_extractor)\n",
    "theme_classifier.accuracy(theme_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
